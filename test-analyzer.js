/**
 * å½¢æ…‹ç´ è§£æžã¨ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ—
 * kuromoji.jsã§æ—¥æœ¬èªžãƒ†ã‚­ã‚¹ãƒˆã‚’è§£æžã—ã€é »å‡ºã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º
 */

import kuromoji from 'kuromoji';
import { fileURLToPath } from 'url';
import { dirname, join } from 'path';
import { fetchHatenaBookmarks } from './test-hatena.js';

const __filename = fileURLToPath(import.meta.url);
const __dirname = dirname(__filename);

// kuromojiè¾žæ›¸ã®ãƒ‘ã‚¹
const DICT_PATH = join(__dirname, 'node_modules', 'kuromoji', 'dict');

/**
 * kuromoji tokenizerã®åˆæœŸåŒ–
 */
function initTokenizer() {
  return new Promise((resolve, reject) => {
    kuromoji.builder({ dicPath: DICT_PATH }).build((err, tokenizer) => {
      if (err) {
        reject(err);
      } else {
        resolve(tokenizer);
      }
    });
  });
}

/**
 * ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰åè©žã‚’æŠ½å‡º
 * @param {object} tokenizer - kuromoji tokenizer
 * @param {string} text - è§£æžå¯¾è±¡ãƒ†ã‚­ã‚¹ãƒˆ
 * @returns {Array} åè©žã®é…åˆ—
 */
function extractNouns(tokenizer, text) {
  if (!text) return [];

  const tokens = tokenizer.tokenize(text);
  const nouns = tokens
    .filter(token => {
      // åè©žã®ã¿ã‚’æŠ½å‡ºï¼ˆpos: å“è©žï¼‰
      const isNoun = token.pos === 'åè©ž';
      // ä¸€èˆ¬åè©žã€å›ºæœ‰åè©žã€ã‚µå¤‰æŽ¥ç¶šã‚’å¯¾è±¡
      const isTargetType = ['ä¸€èˆ¬', 'å›ºæœ‰åè©ž', 'ã‚µå¤‰æŽ¥ç¶š'].includes(token.pos_detail_1);
      // 1æ–‡å­—ã®åè©žã¯é™¤å¤–ï¼ˆãƒŽã‚¤ã‚ºãŒå¤šã„ãŸã‚ï¼‰
      const isLongEnough = token.surface_form.length > 1;

      return isNoun && isTargetType && isLongEnough;
    })
    .map(token => token.surface_form);

  return nouns;
}

/**
 * ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰é »åº¦ã‚«ã‚¦ãƒ³ãƒˆ
 * @param {Array} words - å˜èªžã®é…åˆ—
 * @returns {Map} å˜èªž => å‡ºç¾å›žæ•°ã®Map
 */
function countWords(words) {
  const counts = new Map();

  words.forEach(word => {
    counts.set(word, (counts.get(word) || 0) + 1);
  });

  return counts;
}

/**
 * é »åº¦é †ã«ã‚½ãƒ¼ãƒˆã—ã¦ä¸Šä½ã‚’å–å¾—
 * @param {Map} wordCounts - å˜èªžé »åº¦Map
 * @param {number} topN - ä¸Šä½Nä»¶
 * @returns {Array} [{word, count}]ã®é…åˆ—
 */
function getTopKeywords(wordCounts, topN = 20) {
  return Array.from(wordCounts.entries())
    .map(([word, count]) => ({ word, count }))
    .sort((a, b) => b.count - a.count)
    .slice(0, topN);
}

/**
 * ã¯ã¦ãªãƒ–ãƒƒã‚¯ãƒžãƒ¼ã‚¯ã‹ã‚‰ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºã®ãƒ†ã‚¹ãƒˆ
 */
async function testKeywordExtraction() {
  console.log('ðŸ” ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰æŠ½å‡ºãƒ—ãƒ­ãƒˆã‚¿ã‚¤ãƒ—é–‹å§‹\n');

  try {
    // 1. kuromojiåˆæœŸåŒ–
    console.log('ðŸ“– kuromojiè¾žæ›¸ã‚’èª­ã¿è¾¼ã¿ä¸­...');
    const tokenizer = await initTokenizer();
    console.log('âœ… kuromojiåˆæœŸåŒ–å®Œäº†\n');

    // 2. ã¯ã¦ãªãƒ–ãƒƒã‚¯ãƒžãƒ¼ã‚¯ã‹ã‚‰è¨˜äº‹å–å¾—
    console.log('ðŸ“° ã¯ã¦ãªãƒ–ãƒƒã‚¯ãƒžãƒ¼ã‚¯ã‹ã‚‰è¨˜äº‹å–å¾—ä¸­...');
    const items = await fetchHatenaBookmarks('tech', 20);
    console.log(`âœ… ${items.length}ä»¶ã®è¨˜äº‹ã‚’å–å¾—\n`);

    // 3. ã‚¿ã‚¤ãƒˆãƒ«ã¨æ¦‚è¦ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º
    const allText = items
      .map(item => {
        const title = item.title || '';
        const snippet = item.contentSnippet || '';
        return `${title} ${snippet}`;
      })
      .join(' ');

    console.log(`ðŸ“ æŠ½å‡ºãƒ†ã‚­ã‚¹ãƒˆé•·: ${allText.length}æ–‡å­—\n`);

    // 4. å½¢æ…‹ç´ è§£æž + åè©žæŠ½å‡º
    console.log('ðŸ”¬ å½¢æ…‹ç´ è§£æžå®Ÿè¡Œä¸­...');
    const nouns = extractNouns(tokenizer, allText);
    console.log(`âœ… ${nouns.length}å€‹ã®åè©žã‚’æŠ½å‡º\n`);

    // 5. ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚«ã‚¦ãƒ³ãƒˆ
    const wordCounts = countWords(nouns);
    const topKeywords = getTopKeywords(wordCounts, 30);

    // 6. çµæžœè¡¨ç¤º
    console.log('ðŸ† é »å‡ºã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ TOP 30\n');
    console.log('â”€'.repeat(50));

    topKeywords.forEach((kw, index) => {
      const bar = 'â–ˆ'.repeat(Math.ceil(kw.count / 2));
      console.log(`${(index + 1).toString().padStart(2)}. ${kw.word.padEnd(15)} ${kw.count.toString().padStart(3)}å›ž ${bar}`);
    });

    console.log('\n' + 'â”€'.repeat(50));

    // 7. ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã‚µãƒ³ãƒ—ãƒ«è¡¨ç¤º
    console.log('\nðŸ“Š å½¢æ…‹ç´ è§£æžã‚µãƒ³ãƒ—ãƒ«ï¼ˆæœ€åˆã®10ãƒˆãƒ¼ã‚¯ãƒ³ï¼‰:\n');
    const sampleTokens = tokenizer.tokenize(items[0].title).slice(0, 10);
    console.log(JSON.stringify(sampleTokens, null, 2));

    return { topKeywords, totalNouns: nouns.length, totalItems: items.length };
  } catch (error) {
    console.error('âŒ ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿ:', error);
    throw error;
  }
}

/**
 * ãƒ†ã‚­ã‚¹ãƒˆç›´æŽ¥è§£æžã®ãƒ†ã‚¹ãƒˆ
 */
async function testDirectAnalysis(text) {
  console.log('ðŸ” ãƒ†ã‚­ã‚¹ãƒˆç›´æŽ¥è§£æžãƒ†ã‚¹ãƒˆ\n');

  const tokenizer = await initTokenizer();
  const nouns = extractNouns(tokenizer, text);
  const wordCounts = countWords(nouns);
  const topKeywords = getTopKeywords(wordCounts, 10);

  console.log('å…¥åŠ›ãƒ†ã‚­ã‚¹ãƒˆ:', text);
  console.log('\næŠ½å‡ºã•ã‚ŒãŸåè©ž:', nouns.join(', '));
  console.log('\nTOP 10ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰:');
  topKeywords.forEach(kw => {
    console.log(`  ${kw.word}: ${kw.count}å›ž`);
  });

  return topKeywords;
}

// ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ
if (import.meta.url === `file://${process.argv[1]}`) {
  const directText = process.argv[2];

  if (directText) {
    testDirectAnalysis(directText).catch(console.error);
  } else {
    testKeywordExtraction().catch(console.error);
  }
}

export { initTokenizer, extractNouns, countWords, getTopKeywords };
